---
layout: post
title: LinkedIn Post 9tZO
subtitle: LinkedIn Post
cover-img: /assets/img/forest_path.jpg
tags: [learning, personal, llm, opensource, genai]
---
<!-- Original LinkedIn post: https://www.linkedin.com/posts/activity-7167133058775650304-9tZO -->

üß™ Model merging takes multiple LLMs and combines into one. It's cost-effective (no GPU needed), and can be done on consumer hardware.

üèÜ Merging can result in SOTA models as evaluated on the Hugging Face Open LLM Leaderboard. My neurotic-crown-clown model just ranked 3rd (avg. 76.38) from thousands of 7B models on ü§ó. Whoop! 

üìñ Required absolutely no skill on my part, just time to read and try. As part of my learning journey, I worked through a great blog on merging from Maxime Labonne (https://lnkd.in/eZSgaiy6). I really ‚ù§Ô∏è the knowledge sharing mindset of the open source LLM community, this space is really special and a great place to be.

‚ôüÔ∏è My strategy? Simple, build on the shoulders of giants with strong base models for the merge:
1. NeuralMonarch (https://lnkd.in/ekHpFCYf) by Maxime Labonne
2. AlphaMonarch (https://lnkd.in/eDqBVrCC) by Maxime Labonne 
3. Jaskier-7b-dpo-v5.6 (https://lnkd.in/eYcfxadc) from bards.ai

‚ùì As an aside, is merging merges becoming a way to game eval leaderboards?

Neurotic-crown-clown: https://lnkd.in/esGHfP98
Neurotic-crown-clown GGUF quantised: https://lnkd.in/eaGFf7bG
Neurotic-crown-clown AWQ quantised: https://lnkd.in/e3JfZiXj
Neurotic-crown-clown EXL2 quantised: https://lnkd.in/eG4_tnTa
HF open LLM leaderboard: https://lnkd.in/ehnBtiAg

![](../assets/img/clown.jpg)
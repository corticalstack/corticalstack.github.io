---
layout: post
title: Day 19 of 100 days of large language models
subtitle: API response streaming for reducing perceived LLM latency
cover-img: /assets/img/forest_path.jpg
tags: [learning, personal, 100 days of large language models, natural language processing, machine learning, artificial intelligence]
---
ğŸ¢ LLMs can be slow. Working on a recommender solution, beautiful output but oh my, the latency of API batch!

ğŸš€ Step in API response streaming for reducing perceived latency, as the user sees near real-time LLM progress. Just like the #chatgpt you know.

ğŸ› ï¸ Knocked this streaming demo app up. A basic Streamlit front-end, with #FastAPI (backend), LangChain (LLM orchestration), and #Azure OpenAI (LLM model) all configured with callback handlers and streaming enabled to send partial message deltas back to the client via WebSocket.

ğŸ¥‡ Looking forward to baking this into my recommender for a better customer experience!

ğŸ“ Demo backend is a boiled-down version of main py in Langchain's (chat-langchain repo)[https://github.com/langchain-ai/chat-langchain].

ğŸ“¦ Demo app code (here)[https://github.com/corticalstack/streaming-streamlit-fastapi-langchain-azureopenai].

![](../assets/img/Streamlit stream.gif)
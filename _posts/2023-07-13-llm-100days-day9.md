---
layout: post
title: Day 9 of 100 days of large language models
subtitle: Bigger is better for LLMs, right?
cover-img: /assets/img/forest_path.jpg
tags: [learning, personal, 100 days of large language models, natural language processing, machine learning, artificial intelligence]
---
‚ùì Bigger is better for LLMs, right?

‚ùå Not so. Recent AI research shows smaller models outperforming larger counterparts in many cases.

üåü Meta recently released [LIMA](https://lnkd.in/dCwgMM_b) (Less Is More for Alignment), a 65B (billion parameters, or number of updateable values) model that outperformed GPT-4 in 43% of cases, outperformed Google's Bard by 58%, and by 65% compared with DaVinci003.

üåü DeepMind's Chinchilla 70B outperformed Gopher 280B, GPT-3 175B, Jurassic-1 170B, and Megatron-Turin NLG 530B, on the MMLU benchmark. The [paper by Hoffman et al.](https://arxiv.org/pdf/2203.15556.pdf) showed that the larger models were unnecessarily large and undertrained.

üí° Next time you're designing a solution powered by LLMs, think carefully about the goal. Do you need the model to have knowledge of philosophy and nuclear physics, and be able to carry out many tasks? Or does the model only need to handle something specific like information retrieval or translation?

üí° Many organisations start with a do-it-all, off-the-shelf model like GPT-4. With some good prompt engineering, inference parameter tuning, and few shot inference to give the model some in-context examples, you may achieve just as good performance with a smaller model that you have more control and insight into, for less money.